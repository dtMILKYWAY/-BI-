from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id

# --- é…ç½®ä¿¡æ¯ ---
MINIO_ENDPOINT = "http://localhost:9000"
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin"
POSTGRES_URL = "jdbc:postgresql://localhost:5432/superset"
POSTGRES_USER = "superset"
POSTGRES_PASSWORD = "superset"

def main():
    print("æ­£åœ¨åˆå§‹åŒ–SparkSession (ä½¿ç”¨PySpark 3.2.4)...")
    
    try:
        # 1. åˆå§‹åŒ– SparkSession
        #    ä½¿ç”¨ç»è¿‡éªŒè¯çš„ã€ä¸PySpark 3.2.4æœ€å…¼å®¹çš„ä¾èµ–ç»„åˆ
        spark = (
            SparkSession.builder.appName("SalesETL_v3")
            
            # --- ä¾èµ–åŒ…é…ç½® (è¿™æ˜¯å…³é”®) ---
            .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.postgresql:postgresql:42.5.0")
            # --- æ ¸å¿ƒä¿®å¤ï¼šä¸ºJava 9+ çš„æ¨¡å—åŒ–ç³»ç»Ÿæ·»åŠ --add-openså‚æ•° ---
            .config(
                "spark.driver.extraJavaOptions",
                "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED " +
                "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED"
            )
            
            .getOrCreate()
        )

        # 2. è·å– SparkContext å¹¶è®¾ç½® Hadoop é…ç½®
        #    è¿™æ˜¯ä¿è¯é…ç½®ç”Ÿæ•ˆçš„æœ€é«˜ä¼˜å…ˆçº§æ–¹å¼
        sc = spark.sparkContext
        sc._jsc.hadoopConfiguration().set("fs.s3a.endpoint", MINIO_ENDPOINT)
        sc._jsc.hadoopConfiguration().set("fs.s3a.access.key", MINIO_ACCESS_KEY)
        sc._jsc.hadoopConfiguration().set("fs.s3a.secret.key", MINIO_SECRET_KEY)
        sc._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
        sc._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        sc._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        
        print("SparkSession åˆå§‹åŒ–æˆåŠŸ!")
        
    except Exception as e:
        print(f"SparkSession åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    try:
        # 3. ä»MinIOè¯»å–åŸå§‹æ•°æ®
        print("æ­£åœ¨ä»MinIOè¯»å–åŸå§‹æ•°æ®...")
        raw_df = spark.read.option("header", "true").option("inferSchema", "true").csv("s3a://large-data/sales/sales_data_large_raw.csv")
        print("åŸå§‹æ•°æ®è¯»å–æˆåŠŸï¼Œæ•°æ®é¢„è§ˆï¼š")
        raw_df.show(5)

        # ... (åç»­çš„æ•°æ®å¤„ç†ä»£ç ä¿æŒä¸å˜) ...
        # 4. æ•°æ®è½¬æ¢ä¸æ¸…æ´—
        print("æ­£åœ¨è½¬æ¢æ•°æ®...")
        transformed_df = raw_df.withColumn("TotalPrice", col("Price") * col("Quantity")) \
                               .withColumn("OrderDate", col("OrderDate").cast("date"))

        # 5. åˆ›å»ºç»´åº¦è¡¨å’Œäº‹å®è¡¨
        print("æ­£åœ¨åˆ›å»ºç»´åº¦è¡¨å’Œäº‹å®è¡¨...")
        dim_products = transformed_df.select("ProductID", "ProductName", "Category", "Price").distinct() \
                                     .withColumn("product_key", monotonically_increasing_id())
        print("äº§å“ç»´åº¦è¡¨ (dim_products) é¢„è§ˆï¼š")
        dim_products.show(5)

        fact_sales = transformed_df.join(dim_products, on="ProductID", how="left") \
                                   .select(
                                       col("OrderID").alias("order_id"),
                                       col("UserID").alias("user_id"),
                                       col("product_key"),
                                       col("OrderDate").alias("order_date"),
                                       col("City").alias("city"),
                                       col("Quantity").alias("quantity"),
                                       col("TotalPrice").alias("total_price")
                                   )
        print("é”€å”®äº‹å®è¡¨ (fact_sales) é¢„è§ˆï¼š")
        fact_sales.show(5)

        # 6. å°†å¤„ç†å¥½çš„è¡¨å†™å…¥PostgreSQL
        print("æ­£åœ¨å°†è¡¨å†™å…¥PostgreSQL...")
        db_properties = {
            "user": POSTGRES_USER,
            "password": POSTGRES_PASSWORD,
            "driver": "org.postgresql.Driver"
        }
        dim_products.write.jdbc(url=POSTGRES_URL, table="dim_products", mode="overwrite", properties=db_properties)
        print("âœ… ç»´åº¦è¡¨ 'dim_products' å†™å…¥æˆåŠŸã€‚")

        fact_sales.write.jdbc(url=POSTGRES_URL, table="fact_sales", mode="overwrite", properties=db_properties)
        print("âœ… äº‹å®è¡¨ 'fact_sales' å†™å…¥æˆåŠŸã€‚")

        print("ğŸ‰ ETL æµç¨‹å…¨éƒ¨å®Œæˆ!")

    except Exception as e:
        print(f"ETLå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if 'spark' in locals():
            spark.stop()
            print("SparkSessionå·²åœæ­¢ã€‚")

if __name__ == "__main__":
    main()